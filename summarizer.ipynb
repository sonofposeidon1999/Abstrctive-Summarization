{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zbxhyl_zFlWL"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#tf.enable_eager_execution()\n",
    "import time\n",
    "import re\n",
    "import pickle\n",
    "#from rouge.rouge import rouge_n_sentence_level\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yH5cg5pSIHaZ"
   },
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K_AjGkWXITKA"
   },
   "outputs": [],
   "source": [
    "news = pd.read_csv(\"C:/Users/91908/Desktop/summary1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 15455,
     "status": "ok",
     "timestamp": 1588996050470,
     "user": {
      "displayName": "rohan jagtap",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjFFnpJjw-7WaiTzz7xrkIJjBBwMs5i3OwVVYALIg=s64",
      "userId": "07173842849534370372"
     },
     "user_tz": -330
    },
    "id": "oXtxc-toIc94",
    "outputId": "d39eac33-f967-492d-e6f7-a2331bb638c0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B said yeah it is that the younger they are th...</td>\n",
       "      <td>A asked have you ever served on a jury? B aske...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A said okay some of the things A like to do ar...</td>\n",
       "      <td>A said okay some of the things A like to do ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B said B think for B from alabama. south alaba...</td>\n",
       "      <td>A asked do you want to give a start on it? B s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>last summer B was swim instructor all summer l...</td>\n",
       "      <td>B said B have former exercise program. B am a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A said all right think our experience of campi...</td>\n",
       "      <td>A said all right think our experience of campi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             summary  \\\n",
       "0  B said yeah it is that the younger they are th...   \n",
       "1  A said okay some of the things A like to do ar...   \n",
       "2  B said B think for B from alabama. south alaba...   \n",
       "3  last summer B was swim instructor all summer l...   \n",
       "4  A said all right think our experience of campi...   \n",
       "\n",
       "                                                text  \n",
       "0  A asked have you ever served on a jury? B aske...  \n",
       "1  A said okay some of the things A like to do ar...  \n",
       "2  A asked do you want to give a start on it? B s...  \n",
       "3  B said B have former exercise program. B am a ...  \n",
       "4  A said all right think our experience of campi...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14794,
     "status": "ok",
     "timestamp": 1588996050471,
     "user": {
      "displayName": "rohan jagtap",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjFFnpJjw-7WaiTzz7xrkIJjBBwMs5i3OwVVYALIg=s64",
      "userId": "07173842849534370372"
     },
     "user_tz": -330
    },
    "id": "vR2hg9themaN",
    "outputId": "53a9c60b-3eb2-4f53-9ee5-a427a9f1f1d7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(754, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d4cEp3wmI2BX"
   },
   "outputs": [],
   "source": [
    "document = news['text']\n",
    "summary = news['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'asked',\n",
       " 'have',\n",
       " 'you',\n",
       " 'ever',\n",
       " 'served',\n",
       " 'on',\n",
       " 'a',\n",
       " 'jury?',\n",
       " 'B',\n",
       " 'asked',\n",
       " 'no',\n",
       " 'B',\n",
       " 'have',\n",
       " 'not.',\n",
       " 'B',\n",
       " 'have',\n",
       " 'been',\n",
       " 'called',\n",
       " 'B',\n",
       " 'had',\n",
       " 'to',\n",
       " 'beg',\n",
       " 'off',\n",
       " 'from',\n",
       " 'the',\n",
       " 'duty.',\n",
       " 'you?',\n",
       " 'A',\n",
       " 'said',\n",
       " 'A',\n",
       " 'was',\n",
       " 'called',\n",
       " 'A',\n",
       " 'was',\n",
       " 'not',\n",
       " 'chosen.',\n",
       " 'B',\n",
       " 'said',\n",
       " 'originally',\n",
       " 'chosen',\n",
       " 'primarily',\n",
       " 'B',\n",
       " 'think',\n",
       " 'because',\n",
       " 'B',\n",
       " 'was',\n",
       " 'a',\n",
       " 'young',\n",
       " 'fellow',\n",
       " 'they',\n",
       " 'tend',\n",
       " 'to',\n",
       " 'view',\n",
       " 'the',\n",
       " 'younger',\n",
       " 'fellows',\n",
       " 'as',\n",
       " 'more',\n",
       " 'likely',\n",
       " 'to',\n",
       " 'hand',\n",
       " 'down',\n",
       " 'a',\n",
       " 'guilty',\n",
       " 'verdict.',\n",
       " 'B',\n",
       " 'do',\n",
       " 'not',\n",
       " 'know',\n",
       " 'why.',\n",
       " 'something',\n",
       " 'B',\n",
       " 'picked',\n",
       " 'up',\n",
       " 'in',\n",
       " 'a',\n",
       " 'psychology',\n",
       " 'class',\n",
       " 'some',\n",
       " 'time',\n",
       " 'ago.',\n",
       " 'A',\n",
       " 'asked',\n",
       " 'really?',\n",
       " 'B',\n",
       " 'said',\n",
       " 'yeah',\n",
       " 'it',\n",
       " 'is',\n",
       " 'that',\n",
       " 'the',\n",
       " 'younger',\n",
       " 'they',\n",
       " 'are',\n",
       " 'they',\n",
       " 'tend',\n",
       " 'to',\n",
       " 'be',\n",
       " 'more',\n",
       " 'conservative',\n",
       " 'for',\n",
       " 'some',\n",
       " 'statistical',\n",
       " 'oddball',\n",
       " 'reason',\n",
       " 'they',\n",
       " 'kind',\n",
       " 'of',\n",
       " 'liked',\n",
       " 'me.',\n",
       " 'B',\n",
       " 'looked',\n",
       " 'conser',\n",
       " 'and',\n",
       " 'all',\n",
       " 'that',\n",
       " 'stuff',\n",
       " 'B',\n",
       " 'do',\n",
       " 'not',\n",
       " 'know',\n",
       " 'what',\n",
       " 'they',\n",
       " 'saw',\n",
       " 'in',\n",
       " 'B',\n",
       " 'they',\n",
       " 'saw',\n",
       " 'it.',\n",
       " 'back',\n",
       " 'to',\n",
       " 'the',\n",
       " 'issue',\n",
       " 'B',\n",
       " 'do',\n",
       " 'not',\n",
       " 'know',\n",
       " 'at',\n",
       " 'times',\n",
       " 'B',\n",
       " 'feel',\n",
       " 'that',\n",
       " 'a',\n",
       " 'unanimous',\n",
       " 'decision',\n",
       " 'is',\n",
       " 'warranted',\n",
       " 'especially',\n",
       " 'in',\n",
       " 'cases',\n",
       " 'in',\n",
       " 'which',\n",
       " 'there',\n",
       " 'is',\n",
       " 'no',\n",
       " 'smoking',\n",
       " 'gun.',\n",
       " 'there',\n",
       " 'too',\n",
       " 'there',\n",
       " 'are',\n",
       " 'also',\n",
       " 'cases',\n",
       " 'in',\n",
       " 'which',\n",
       " 'B',\n",
       " 'feel',\n",
       " 'a',\n",
       " 'majority',\n",
       " 'rule',\n",
       " 'might',\n",
       " 'be',\n",
       " 'acceptable',\n",
       " 'particularly',\n",
       " 'B',\n",
       " 'think',\n",
       " 'in',\n",
       " 'civil',\n",
       " 'cases.',\n",
       " 'in',\n",
       " 'criminal',\n",
       " 'cases',\n",
       " 'B',\n",
       " 'had',\n",
       " 'like',\n",
       " 'to',\n",
       " 'see',\n",
       " 'the',\n",
       " 'remain',\n",
       " 'in',\n",
       " 'civil',\n",
       " 'cases',\n",
       " 'B',\n",
       " 'think',\n",
       " 'a',\n",
       " 'majority',\n",
       " 'rule',\n",
       " 'jury',\n",
       " 'would',\n",
       " 'be',\n",
       " 'sufficient.',\n",
       " 'A',\n",
       " 'asked',\n",
       " 'could',\n",
       " 'you',\n",
       " 'give',\n",
       " 'A',\n",
       " 'an',\n",
       " 'example',\n",
       " 'of',\n",
       " 'a',\n",
       " 'case',\n",
       " 'where',\n",
       " 'you',\n",
       " 'think',\n",
       " 'that?',\n",
       " 'B',\n",
       " 'said',\n",
       " 'in',\n",
       " 'a',\n",
       " 'criminal',\n",
       " 'case',\n",
       " 'say',\n",
       " 'one',\n",
       " 'in',\n",
       " 'which',\n",
       " 'there',\n",
       " 'is',\n",
       " 'say',\n",
       " 'assault',\n",
       " 'or',\n",
       " 'some',\n",
       " 'such.',\n",
       " 'B',\n",
       " 'think',\n",
       " 'there',\n",
       " 'ought',\n",
       " 'to',\n",
       " 'be',\n",
       " 'a',\n",
       " 'unanimous',\n",
       " 'vote',\n",
       " 'because',\n",
       " 'by',\n",
       " 'law',\n",
       " 'and',\n",
       " 'by',\n",
       " 'constitution',\n",
       " 'there',\n",
       " 'must',\n",
       " 'be',\n",
       " 'proven',\n",
       " 'beyond',\n",
       " 'a',\n",
       " 'shadow',\n",
       " 'of',\n",
       " 'a',\n",
       " 'reasonable',\n",
       " 'doubt',\n",
       " 'that',\n",
       " 'the',\n",
       " 'person',\n",
       " 'in',\n",
       " 'question',\n",
       " 'did',\n",
       " 'this.',\n",
       " 'in',\n",
       " 'civil',\n",
       " 'law',\n",
       " 'there',\n",
       " 'is',\n",
       " 'such',\n",
       " 'a',\n",
       " 'thing',\n",
       " 'as',\n",
       " 'let',\n",
       " 'us',\n",
       " 'say',\n",
       " 'misappropriations',\n",
       " 'or',\n",
       " 'misuse',\n",
       " 'of',\n",
       " 'financial',\n",
       " 'instruments',\n",
       " 'or',\n",
       " 'something',\n",
       " 'like',\n",
       " 'that.',\n",
       " 'a',\n",
       " 'majority',\n",
       " 'rule',\n",
       " 'B',\n",
       " 'think',\n",
       " 'would',\n",
       " 'be',\n",
       " 'more',\n",
       " 'in',\n",
       " 'line',\n",
       " 'as',\n",
       " 'there',\n",
       " 'is',\n",
       " 'no',\n",
       " 'real',\n",
       " 'smoking',\n",
       " 'gun',\n",
       " 'in',\n",
       " 'the',\n",
       " 'civil',\n",
       " 'cases.',\n",
       " 'B',\n",
       " 'do',\n",
       " 'not',\n",
       " 'know',\n",
       " 'if',\n",
       " 'B',\n",
       " 'am',\n",
       " 'making',\n",
       " 'any',\n",
       " 'sense',\n",
       " 'or',\n",
       " 'not.',\n",
       " 'A',\n",
       " 'asked',\n",
       " 'A',\n",
       " 'think',\n",
       " 'are',\n",
       " 'you',\n",
       " 'just',\n",
       " 'trying',\n",
       " 'to',\n",
       " 'say',\n",
       " 'that',\n",
       " 'criminal',\n",
       " 'cases',\n",
       " 'are',\n",
       " 'more',\n",
       " 'tangible',\n",
       " 'B',\n",
       " 'said',\n",
       " 'yeah',\n",
       " 'often',\n",
       " 'there',\n",
       " 'is',\n",
       " 'more',\n",
       " 'incriminating',\n",
       " 'evidence.',\n",
       " 'for',\n",
       " 'instance',\n",
       " 'say',\n",
       " 'also',\n",
       " 'too',\n",
       " 'B',\n",
       " 'think',\n",
       " 'tempering',\n",
       " 'this',\n",
       " 'and',\n",
       " 'the',\n",
       " 'fact',\n",
       " 'that',\n",
       " 'the',\n",
       " 'consequences',\n",
       " 'are',\n",
       " 'much',\n",
       " 'more',\n",
       " 'serious',\n",
       " 'in',\n",
       " 'a',\n",
       " 'criminal',\n",
       " 'case.',\n",
       " 'A',\n",
       " 'asked',\n",
       " 'do',\n",
       " 'you',\n",
       " 'think',\n",
       " 'in',\n",
       " 'a',\n",
       " 'civil',\n",
       " 'case',\n",
       " 'if',\n",
       " 'there',\n",
       " 'was',\n",
       " 'majority',\n",
       " 'rule',\n",
       " 'that',\n",
       " 'it',\n",
       " 'would',\n",
       " 'be',\n",
       " 'easy',\n",
       " 'for',\n",
       " 'someone',\n",
       " 'to',\n",
       " 'be',\n",
       " 'set',\n",
       " 'up?',\n",
       " 'B',\n",
       " 'asked',\n",
       " 'B',\n",
       " 'really',\n",
       " 'cannot',\n",
       " 'say',\n",
       " 'for',\n",
       " 'certain',\n",
       " 'truth',\n",
       " 'be',\n",
       " 'known.',\n",
       " 'as',\n",
       " 'it',\n",
       " 'stands',\n",
       " 'many',\n",
       " 'ways',\n",
       " 'and',\n",
       " 'means',\n",
       " 'by',\n",
       " 'which',\n",
       " 'a',\n",
       " 'person',\n",
       " 'can',\n",
       " 'be',\n",
       " 'set',\n",
       " 'up',\n",
       " 'both',\n",
       " 'in',\n",
       " 'a',\n",
       " 'A',\n",
       " 'said',\n",
       " 'no',\n",
       " 'A',\n",
       " 'am',\n",
       " 'not.',\n",
       " 'B',\n",
       " 'said',\n",
       " 'a',\n",
       " 'fellow',\n",
       " 'when',\n",
       " 'he',\n",
       " 'was',\n",
       " 'much',\n",
       " 'younger',\n",
       " 'was',\n",
       " 'tried',\n",
       " 'and',\n",
       " 'convicted',\n",
       " 'and',\n",
       " 'sentenced',\n",
       " 'to',\n",
       " 'death.',\n",
       " 'fortunately',\n",
       " 'in',\n",
       " 'his',\n",
       " 'case',\n",
       " 'the',\n",
       " 'death',\n",
       " 'penalty',\n",
       " 'was',\n",
       " 'revoked',\n",
       " 'he',\n",
       " 'served',\n",
       " 'out',\n",
       " 'sentence',\n",
       " 'until',\n",
       " 'it',\n",
       " 'was',\n",
       " 'discovered',\n",
       " 'by',\n",
       " 'a',\n",
       " 'fellow',\n",
       " 'who',\n",
       " 'was',\n",
       " 'making',\n",
       " 'a',\n",
       " 'documentary',\n",
       " 'called',\n",
       " 'the',\n",
       " 'thin',\n",
       " 'blue',\n",
       " 'line',\n",
       " 'that',\n",
       " 'this',\n",
       " 'guy',\n",
       " 'had',\n",
       " 'basically',\n",
       " 'gotten',\n",
       " 'railroaded',\n",
       " 'through',\n",
       " 'the',\n",
       " 'judicial',\n",
       " 'system.',\n",
       " 'the',\n",
       " 'case',\n",
       " 'was',\n",
       " 'reopened',\n",
       " 'he',\n",
       " 'was',\n",
       " 'exonerated.',\n",
       " 'A',\n",
       " 'said',\n",
       " 'A',\n",
       " 'think',\n",
       " 'that',\n",
       " 'there',\n",
       " 'are',\n",
       " 'many',\n",
       " 'cases',\n",
       " 'in',\n",
       " 'our',\n",
       " 'judicial',\n",
       " 'system',\n",
       " 'where',\n",
       " 'justice',\n",
       " 'is',\n",
       " 'not',\n",
       " 'served.',\n",
       " 'B',\n",
       " 'said',\n",
       " 'yeah',\n",
       " 'many',\n",
       " 'laws',\n",
       " 'but',\n",
       " 'little',\n",
       " 'justice.',\n",
       " 'A',\n",
       " 'said',\n",
       " 'A',\n",
       " 'also',\n",
       " 'think',\n",
       " 'just',\n",
       " 'like',\n",
       " 'you',\n",
       " 'were',\n",
       " 'talking',\n",
       " 'about',\n",
       " 'before',\n",
       " 'why',\n",
       " 'you',\n",
       " 'were',\n",
       " 'chosen',\n",
       " 'to',\n",
       " 'be',\n",
       " 'on',\n",
       " 'a',\n",
       " 'jury',\n",
       " 'that',\n",
       " 'process',\n",
       " 'of',\n",
       " 'picking',\n",
       " 'jurors',\n",
       " 'is',\n",
       " 'not',\n",
       " 'always',\n",
       " 'objective.',\n",
       " 'B',\n",
       " 'said',\n",
       " 'certainly',\n",
       " 'not',\n",
       " 'certainly',\n",
       " 'not.',\n",
       " 'they',\n",
       " 'like',\n",
       " 'to',\n",
       " 'think',\n",
       " 'that',\n",
       " 'they',\n",
       " 'are',\n",
       " 'getting',\n",
       " 'someone',\n",
       " 'who',\n",
       " 'is',\n",
       " 'objective',\n",
       " 'in',\n",
       " 'all',\n",
       " 'this',\n",
       " 'they',\n",
       " 'are',\n",
       " 'really',\n",
       " 'looking',\n",
       " 'for',\n",
       " 'someone',\n",
       " 'who',\n",
       " 'will',\n",
       " 'pretty',\n",
       " 'much',\n",
       " 'fulfill',\n",
       " 'the',\n",
       " \"lawyers'\",\n",
       " 'desires.',\n",
       " 'you',\n",
       " 'get',\n",
       " 'up',\n",
       " 'they',\n",
       " 'ask',\n",
       " 'you',\n",
       " 'a',\n",
       " 'few',\n",
       " 'questions',\n",
       " 'both',\n",
       " 'sides',\n",
       " 'do',\n",
       " 'either',\n",
       " 'challenged',\n",
       " 'which',\n",
       " 'is',\n",
       " 'each',\n",
       " 'attorney',\n",
       " 'can',\n",
       " 'use',\n",
       " 'that',\n",
       " 'as',\n",
       " 'much',\n",
       " 'as',\n",
       " 'they',\n",
       " 'like',\n",
       " 'B',\n",
       " 'think',\n",
       " 'it',\n",
       " 'is',\n",
       " 'a',\n",
       " 'limit',\n",
       " 'now',\n",
       " 'they',\n",
       " 'probably',\n",
       " 'have',\n",
       " 'a',\n",
       " 'limit',\n",
       " 'now',\n",
       " 'they',\n",
       " 'pretty',\n",
       " 'much',\n",
       " 'go',\n",
       " 'through',\n",
       " 'that',\n",
       " 'you',\n",
       " 'have',\n",
       " 'to',\n",
       " 'give',\n",
       " 'a',\n",
       " 'reason',\n",
       " 'to',\n",
       " 'the',\n",
       " 'court',\n",
       " 'why',\n",
       " 'you',\n",
       " 'cannot',\n",
       " 'serve.',\n",
       " 'for',\n",
       " 'B',\n",
       " 'it',\n",
       " 'was',\n",
       " 'financial',\n",
       " 'hardship',\n",
       " 'onto',\n",
       " 'the',\n",
       " 'thing',\n",
       " 'B',\n",
       " 'was',\n",
       " 'never',\n",
       " 'aware',\n",
       " 'that',\n",
       " 'juries',\n",
       " 'had',\n",
       " 'any',\n",
       " 'say',\n",
       " 'on',\n",
       " 'recommending',\n",
       " 'sentencing.',\n",
       " 'it',\n",
       " 'was',\n",
       " 'always',\n",
       " 'B',\n",
       " 'impression',\n",
       " 'that',\n",
       " 'the',\n",
       " 'justice',\n",
       " 'himself',\n",
       " 'or',\n",
       " 'herself',\n",
       " 'had',\n",
       " 'the',\n",
       " 'final',\n",
       " 'say.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document[0].split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f8gKyq1gIq4r"
   },
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12201,
     "status": "ok",
     "timestamp": 1588996050475,
     "user": {
      "displayName": "rohan jagtap",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjFFnpJjw-7WaiTzz7xrkIJjBBwMs5i3OwVVYALIg=s64",
      "userId": "07173842849534370372"
     },
     "user_tz": -330
    },
    "id": "TJ6LE4MrJjC_",
    "outputId": "cb4f9812-a59f-4bfc-9111-e95c6c72c5f6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    <go> B said yeah it is that the younger they a...\n",
       "1    <go> A said okay some of the things A like to ...\n",
       "2    <go> B said B think for B from alabama. south ...\n",
       "3    <go> last summer B was swim instructor all sum...\n",
       "4    <go> A said all right think our experience of ...\n",
       "Name: summary, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for decoder sequence\n",
    "summary = summary.apply(lambda x: '<go> ' + x + ' <stop>')\n",
    "summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict = {}\n",
    "with open(\"glove.6B.50d.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        embeddings_dict[word] = vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "95Zv7FIvKbTi"
   },
   "source": [
    "#### Tokenizing the texts into integer tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7TqbpEyPMRqa"
   },
   "outputs": [],
   "source": [
    "# since < and > from default tokens cannot be removed\n",
    "filters = '!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n'\n",
    "oov_token = '<unk>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cHw2csoYImsa"
   },
   "outputs": [],
   "source": [
    "document_tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token=oov_token)\n",
    "summary_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=filters, oov_token=oov_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DWU9Xu7OKVab"
   },
   "outputs": [],
   "source": [
    "document_tokenizer.fit_on_texts(document)\n",
    "summary_tokenizer.fit_on_texts(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3ESm-aYR-tvx"
   },
   "outputs": [],
   "source": [
    "inputs = document_tokenizer.texts_to_sequences(document)\n",
    "targets = summary_tokenizer.texts_to_sequences(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "462.9575596816976"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(i) for i in targets])/len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1240,
     "status": "ok",
     "timestamp": 1588996264681,
     "user": {
      "displayName": "rohan jagtap",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjFFnpJjw-7WaiTzz7xrkIJjBBwMs5i3OwVVYALIg=s64",
      "userId": "07173842849534370372"
     },
     "user_tz": -330
    },
    "id": "kVyErXAei5_b",
    "outputId": "41027c54-ce96-46d2-85c1-ff09bfe5d7dd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[55, 8, 2, 978]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_tokenizer.texts_to_sequences([\"This is a test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1268,
     "status": "ok",
     "timestamp": 1588996308592,
     "user": {
      "displayName": "rohan jagtap",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjFFnpJjw-7WaiTzz7xrkIJjBBwMs5i3OwVVYALIg=s64",
      "userId": "07173842849534370372"
     },
     "user_tz": -330
    },
    "id": "Ryx9qx90jwXu",
    "outputId": "c4b8270a-e497-47a5-e93b-90a328a7f0a9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['okay just them will']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_tokenizer.sequences_to_texts([[184, 22, 46, 71]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1311,
     "status": "ok",
     "timestamp": 1588996326633,
     "user": {
      "displayName": "rohan jagtap",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjFFnpJjw-7WaiTzz7xrkIJjBBwMs5i3OwVVYALIg=s64",
      "userId": "07173842849534370372"
     },
     "user_tz": -330
    },
    "id": "KoizyBvLKv8h",
    "outputId": "62198745-1a53-41ef-f10c-5c9ca315fbe4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17981, 12135)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_vocab_size = len(document_tokenizer.word_index) + 1\n",
    "decoder_vocab_size = len(summary_tokenizer.word_index) + 1\n",
    "\n",
    "# vocab_size\n",
    "encoder_vocab_size, decoder_vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mZden_q9_eZr"
   },
   "source": [
    "#### Obtaining insights on lengths for defining maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ma4o2nGdK5Xb"
   },
   "outputs": [],
   "source": [
    "document_lengths = pd.Series([len(x.split()) for x in document])\n",
    "summary_lengths = pd.Series([len(x.split()) for x in summary])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1365,
     "status": "ok",
     "timestamp": 1588996333925,
     "user": {
      "displayName": "rohan jagtap",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjFFnpJjw-7WaiTzz7xrkIJjBBwMs5i3OwVVYALIg=s64",
      "userId": "07173842849534370372"
     },
     "user_tz": -330
    },
    "id": "iXZlO99C-UXK",
    "outputId": "8b1480e7-5481-43f0-b685-e1deecda4548"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     754.000000\n",
       "mean     1080.667109\n",
       "std       376.596493\n",
       "min       455.000000\n",
       "25%       787.000000\n",
       "50%       956.000000\n",
       "75%      1344.750000\n",
       "max      2310.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_lengths.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1139,
     "status": "ok",
     "timestamp": 1588996333927,
     "user": {
      "displayName": "rohan jagtap",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjFFnpJjw-7WaiTzz7xrkIJjBBwMs5i3OwVVYALIg=s64",
      "userId": "07173842849534370372"
     },
     "user_tz": -330
    },
    "id": "ALMwKMx--ZF7",
    "outputId": "df49e521-f058-466e-ec7c-f667c215c748"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     754.000000\n",
       "mean      464.393899\n",
       "std       200.763648\n",
       "min       155.000000\n",
       "25%       325.000000\n",
       "50%       410.000000\n",
       "75%       578.750000\n",
       "max      1156.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_lengths.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cVeMilXr-bpC"
   },
   "outputs": [],
   "source": [
    "# maxlen\n",
    "# taking values > and round figured to 75th percentile\n",
    "# at the same time not leaving high variance\n",
    "encoder_maxlen = 900\n",
    "decoder_maxlen = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_SWap3YJBk-D"
   },
   "source": [
    "#### Padding/Truncating sequences for identical sequence lengths\n",
    "Padding can also be applied to the end of the sequences, which may be more appropriate for some problem domains.\n",
    "Post-sequence padding can be specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vEyUBeu7ACRt"
   },
   "outputs": [],
   "source": [
    "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, maxlen=encoder_maxlen, padding='post', truncating='post')\n",
    "targets = tf.keras.preprocessing.sequence.pad_sequences(targets, maxlen=decoder_maxlen, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wIP0kIIcB8Rm"
   },
   "source": [
    "### Creating dataset pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LzO6l3-AB7hJ"
   },
   "outputs": [],
   "source": [
    "inputs = tf.cast(inputs, dtype=tf.int32)\n",
    "targets = tf.cast(targets, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "slZ5f4P4DurS"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the help of tf.data.Dataset.from_tensor_slices() method, we can get the slices of an array in the form of objects by using tf.data.Dataset.from_tensor_slices() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wI-fV7eABWN6"
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((inputs, targets)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "isN1CpAXLfsl"
   },
   "source": [
    "### Positional Encoding for adding notion of position among words as unlike RNN the Transformer architecture ditched the recurrence mechanism in favor of\n",
    "\n",
    "- Since no order of flow for each word of the sentence so the model adds positional encoding\n",
    "- D dimensional vector that contains info about the specific position in a sentence and is used to equip each word with information about its position in a sentence and not the model itselfNN this is non-directional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Purv7oyhETDZ"
   },
   "outputs": [],
   "source": [
    "def get_angles(position, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    return position * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "40J2pc2NEXp5"
   },
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(\n",
    "        np.arange(position)[:, np.newaxis],\n",
    "        np.arange(d_model)[np.newaxis, :],\n",
    "        d_model\n",
    "    )\n",
    "\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "24Pe01DMMWHc"
   },
   "source": [
    "- Padding mask for masking \"pad\" sequences.\n",
    "\n",
    "- Mask all the pad tokens in the batch of sequence. It ensures that the model does not treat padding as the input. The mask indicates where pad value 0 is present: it outputs a 1 at those locations, and a 0 otherwise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hN1wVQAdMVYy"
   },
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The look-ahead mask is used to mask the future tokens in a sequence. In other words, the mask indicates which entries should not be used.\n",
    "\n",
    "- This means that to predict the third word, only the first and second word will be used. Similarly to predict the fourth word, only the first, second and the third word will be used and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UmjAPLWuMREE"
   },
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n8DqUBc4NFOy"
   },
   "source": [
    "### Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WfknVF7hNKf7"
   },
   "source": [
    "#### Scaled Dot Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w_B6M9OBNBKB"
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rf7_a5uQOfJk"
   },
   "source": [
    "#### Multi-Headed Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iIuFrdXnNZEC"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
    "        output = self.dense(concat_attention)\n",
    "            \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A49tXMVvOkOZ"
   },
   "source": [
    "### Feed Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d9-qoKuTNwKq"
   },
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),\n",
    "        tf.keras.layers.Dense(d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B2RRmn2bOpW9"
   },
   "source": [
    "#### Fundamental Unit of Transformer encoder\n",
    "- The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. \n",
    "- The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network. We employ a residual connection around each of the two sub-layers, followed by layer normalization. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. \n",
    "- To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HNuoJoFWO335"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, training, mask):\n",
    "        attn_output, _ = self.mha(x, x, x, mask)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "        return out2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9i6Zh8gnPqdW"
   },
   "source": [
    "#### Fundamental Unit of Transformer decoder\n",
    "- The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack.\n",
    "- Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. \n",
    "- We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. \n",
    "- This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7CVmvs6dPMRC"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    \n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)\n",
    "\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6zt5MUc_QNid"
   },
   "source": [
    "#### Encoder consisting of multiple EncoderLayer(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BrbnTwijQJ-h"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "    def call(self, x, training, mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "    \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "    \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4N5LrNrvRexg"
   },
   "source": [
    "#### Decoder consisting of multiple DecoderLayer(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UmeqkZrIRbSB"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n",
    "\n",
    "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "    \n",
    "        return x, attention_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lbMNK_bzSHnh"
   },
   "source": [
    "#### Finally, the Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FXHRG-o4R9Mc"
   },
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "    \n",
    "    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
    "\n",
    "        dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "        final_output = self.final_layer(dec_output)\n",
    "\n",
    "        return final_output, attention_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UndsMPZXTdSr"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lMTZJdIoSbuy"
   },
   "outputs": [],
   "source": [
    "# hyper-params\n",
    "num_layers = 2\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "EPOCHS = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uOGvkYDNTjIj"
   },
   "source": [
    "#### Adam optimizer with custom learning rate scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tfiynCLlTL8C"
   },
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DsVdrENTUERY"
   },
   "source": [
    "#### Defining losses and other metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ip1-943kTXXK"
   },
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ktKwyvKtTvF6"
   },
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uW4LA_45T4Aa"
   },
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ze0u6xxXT7dI"
   },
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9XvKy3v6ULnO"
   },
   "source": [
    "#### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d5-RcxqFUCuk"
   },
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    num_layers, \n",
    "    d_model, \n",
    "    num_heads, \n",
    "    dff,\n",
    "    encoder_vocab_size, \n",
    "    decoder_vocab_size, \n",
    "    pe_input=encoder_vocab_size, \n",
    "    pe_target=decoder_vocab_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f56BGiVXU_Dk"
   },
   "source": [
    "#### Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FZxHuyZxU5Pa"
   },
   "outputs": [],
   "source": [
    "def create_masks(inp, tar):\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "  \n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SYIotvaBVI0d"
   },
   "source": [
    "#### Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7786,
     "status": "ok",
     "timestamp": 1588996357748,
     "user": {
      "displayName": "rohan jagtap",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjFFnpJjw-7WaiTzz7xrkIJjBBwMs5i3OwVVYALIg=s64",
      "userId": "07173842849534370372"
     },
     "user_tz": -330
    },
    "id": "tOc1_3c-VGaL",
    "outputId": "eeab15d7-f887-4f37-dfec-c980948f97d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint restored!!\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"checkpoints\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WfpI0gS4c06c"
   },
   "source": [
    "#### Training steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xmVOMzkrczgl"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, tar):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer(\n",
    "            inp, tar_inp, \n",
    "            True, \n",
    "            enc_padding_mask, \n",
    "            combined_mask, \n",
    "            dec_padding_mask\n",
    "        )\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "    train_loss(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6000532,
     "status": "ok",
     "timestamp": 1589005387811,
     "user": {
      "displayName": "rohan jagtap",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjFFnpJjw-7WaiTzz7xrkIJjBBwMs5i3OwVVYALIg=s64",
      "userId": "07173842849534370372"
     },
     "user_tz": -330
    },
    "id": "xORKpv69dSW5",
    "outputId": "572d3232-3af4-4bd6-ebc7-a151f6ad000d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 1.1093\n",
      "Epoch 1 Batch 1 Loss 1.1414\n",
      "Epoch 1 Batch 2 Loss 1.1498\n",
      "Epoch 1 Batch 3 Loss 1.1622\n",
      "Epoch 1 Batch 4 Loss 1.1729\n",
      "Epoch 1 Batch 5 Loss 1.1839\n",
      "Epoch 1 Batch 6 Loss 1.1804\n",
      "Epoch 1 Batch 7 Loss 1.1787\n",
      "Epoch 1 Batch 8 Loss 1.1776\n",
      "Epoch 1 Batch 9 Loss 1.1815\n",
      "Epoch 1 Batch 10 Loss 1.1787\n",
      "Epoch 1 Batch 11 Loss 1.1846\n",
      "Epoch 1 Loss 1.1846\n",
      "Time taken for 1 epoch: 321.42355728149414 secs\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.1232\n",
      "Epoch 2 Batch 1 Loss 1.1511\n",
      "Epoch 2 Batch 2 Loss 1.1503\n",
      "Epoch 2 Batch 3 Loss 1.1424\n",
      "Epoch 2 Batch 4 Loss 1.1492\n",
      "Epoch 2 Batch 5 Loss 1.1523\n",
      "Epoch 2 Batch 6 Loss 1.1558\n",
      "Epoch 2 Batch 7 Loss 1.1572\n",
      "Epoch 2 Batch 8 Loss 1.1609\n",
      "Epoch 2 Batch 9 Loss 1.1683\n",
      "Epoch 2 Batch 10 Loss 1.1732\n",
      "Epoch 2 Batch 11 Loss 1.1736\n",
      "Epoch 2 Loss 1.1736\n",
      "Time taken for 1 epoch: 271.64083552360535 secs\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.0792\n",
      "Epoch 3 Batch 1 Loss 1.1007\n",
      "Epoch 3 Batch 2 Loss 1.1079\n",
      "Epoch 3 Batch 3 Loss 1.1254\n",
      "Epoch 3 Batch 4 Loss 1.1262\n",
      "Epoch 3 Batch 5 Loss 1.1377\n",
      "Epoch 3 Batch 6 Loss 1.1425\n",
      "Epoch 3 Batch 7 Loss 1.1490\n",
      "Epoch 3 Batch 8 Loss 1.1543\n",
      "Epoch 3 Batch 9 Loss 1.1565\n",
      "Epoch 3 Batch 10 Loss 1.1557\n",
      "Epoch 3 Batch 11 Loss 1.1591\n",
      "Epoch 3 Loss 1.1591\n",
      "Time taken for 1 epoch: 287.4504590034485 secs\n",
      "\n",
      "Epoch 4 Batch 0 Loss 1.1649\n",
      "Epoch 4 Batch 1 Loss 1.1474\n",
      "Epoch 4 Batch 2 Loss 1.1517\n",
      "Epoch 4 Batch 3 Loss 1.1336\n",
      "Epoch 4 Batch 4 Loss 1.1447\n",
      "Epoch 4 Batch 5 Loss 1.1390\n",
      "Epoch 4 Batch 6 Loss 1.1279\n",
      "Epoch 4 Batch 7 Loss 1.1311\n",
      "Epoch 4 Batch 8 Loss 1.1363\n",
      "Epoch 4 Batch 9 Loss 1.1368\n",
      "Epoch 4 Batch 10 Loss 1.1398\n",
      "Epoch 4 Batch 11 Loss 1.1382\n",
      "Epoch 4 Loss 1.1382\n",
      "Time taken for 1 epoch: 297.5380907058716 secs\n",
      "\n",
      "Epoch 5 Batch 0 Loss 1.1533\n",
      "Epoch 5 Batch 1 Loss 1.1132\n",
      "Epoch 5 Batch 2 Loss 1.1250\n",
      "Epoch 5 Batch 3 Loss 1.1322\n",
      "Epoch 5 Batch 4 Loss 1.1260\n",
      "Epoch 5 Batch 5 Loss 1.1220\n",
      "Epoch 5 Batch 6 Loss 1.1203\n",
      "Epoch 5 Batch 7 Loss 1.1173\n",
      "Epoch 5 Batch 8 Loss 1.1155\n",
      "Epoch 5 Batch 9 Loss 1.1151\n",
      "Epoch 5 Batch 10 Loss 1.1174\n",
      "Epoch 5 Batch 11 Loss 1.1228\n",
      "Saving checkpoint for epoch 5 at checkpoints\\ckpt-43\n",
      "Epoch 5 Loss 1.1228\n",
      "Time taken for 1 epoch: 297.4471879005432 secs\n",
      "\n",
      "Epoch 6 Batch 0 Loss 1.0928\n",
      "Epoch 6 Batch 1 Loss 1.1053\n",
      "Epoch 6 Batch 2 Loss 1.1081\n",
      "Epoch 6 Batch 3 Loss 1.1051\n",
      "Epoch 6 Batch 4 Loss 1.1072\n",
      "Epoch 6 Batch 5 Loss 1.1041\n",
      "Epoch 6 Batch 6 Loss 1.0997\n",
      "Epoch 6 Batch 7 Loss 1.1084\n",
      "Epoch 6 Batch 8 Loss 1.1150\n",
      "Epoch 6 Batch 9 Loss 1.1148\n",
      "Epoch 6 Batch 10 Loss 1.1168\n",
      "Epoch 6 Batch 11 Loss 1.1162\n",
      "Epoch 6 Loss 1.1162\n",
      "Time taken for 1 epoch: 296.3572382926941 secs\n",
      "\n",
      "Epoch 7 Batch 0 Loss 1.1371\n",
      "Epoch 7 Batch 1 Loss 1.1290\n",
      "Epoch 7 Batch 2 Loss 1.1399\n",
      "Epoch 7 Batch 3 Loss 1.1258\n",
      "Epoch 7 Batch 4 Loss 1.1090\n",
      "Epoch 7 Batch 5 Loss 1.1076\n",
      "Epoch 7 Batch 6 Loss 1.1090\n",
      "Epoch 7 Batch 7 Loss 1.1036\n",
      "Epoch 7 Batch 8 Loss 1.0979\n",
      "Epoch 7 Batch 9 Loss 1.1027\n",
      "Epoch 7 Batch 10 Loss 1.1000\n",
      "Epoch 7 Batch 11 Loss 1.1015\n",
      "Epoch 7 Loss 1.1015\n",
      "Time taken for 1 epoch: 278.7706415653229 secs\n",
      "\n",
      "Epoch 8 Batch 0 Loss 1.0846\n",
      "Epoch 8 Batch 1 Loss 1.0788\n",
      "Epoch 8 Batch 2 Loss 1.0771\n",
      "Epoch 8 Batch 3 Loss 1.0912\n",
      "Epoch 8 Batch 4 Loss 1.0886\n",
      "Epoch 8 Batch 5 Loss 1.0852\n",
      "Epoch 8 Batch 6 Loss 1.0837\n",
      "Epoch 8 Batch 7 Loss 1.0874\n",
      "Epoch 8 Batch 8 Loss 1.0878\n",
      "Epoch 8 Batch 9 Loss 1.0879\n",
      "Epoch 8 Batch 10 Loss 1.0875\n",
      "Epoch 8 Batch 11 Loss 1.0862\n",
      "Epoch 8 Loss 1.0862\n",
      "Time taken for 1 epoch: 270.6894037723541 secs\n",
      "\n",
      "Epoch 9 Batch 0 Loss 1.0717\n",
      "Epoch 9 Batch 1 Loss 1.0706\n",
      "Epoch 9 Batch 2 Loss 1.0577\n",
      "Epoch 9 Batch 3 Loss 1.0684\n",
      "Epoch 9 Batch 4 Loss 1.0753\n",
      "Epoch 9 Batch 5 Loss 1.0739\n",
      "Epoch 9 Batch 6 Loss 1.0671\n",
      "Epoch 9 Batch 7 Loss 1.0634\n",
      "Epoch 9 Batch 8 Loss 1.0606\n",
      "Epoch 9 Batch 9 Loss 1.0651\n",
      "Epoch 9 Batch 10 Loss 1.0722\n",
      "Epoch 9 Batch 11 Loss 1.0741\n",
      "Epoch 9 Loss 1.0741\n",
      "Time taken for 1 epoch: 270.66590666770935 secs\n",
      "\n",
      "Epoch 10 Batch 0 Loss 1.0271\n",
      "Epoch 10 Batch 1 Loss 1.0436\n",
      "Epoch 10 Batch 2 Loss 1.0354\n",
      "Epoch 10 Batch 3 Loss 1.0409\n",
      "Epoch 10 Batch 4 Loss 1.0635\n",
      "Epoch 10 Batch 5 Loss 1.0619\n",
      "Epoch 10 Batch 6 Loss 1.0508\n",
      "Epoch 10 Batch 7 Loss 1.0511\n",
      "Epoch 10 Batch 8 Loss 1.0549\n",
      "Epoch 10 Batch 9 Loss 1.0593\n",
      "Epoch 10 Batch 10 Loss 1.0610\n",
      "Epoch 10 Batch 11 Loss 1.0591\n",
      "Saving checkpoint for epoch 10 at checkpoints\\ckpt-44\n",
      "Epoch 10 Loss 1.0591\n",
      "Time taken for 1 epoch: 270.34778881073 secs\n",
      "\n",
      "Epoch 11 Batch 0 Loss 0.9714\n",
      "Epoch 11 Batch 1 Loss 1.0222\n",
      "Epoch 11 Batch 2 Loss 1.0197\n",
      "Epoch 11 Batch 3 Loss 1.0194\n",
      "Epoch 11 Batch 4 Loss 1.0206\n",
      "Epoch 11 Batch 5 Loss 1.0228\n",
      "Epoch 11 Batch 6 Loss 1.0282\n",
      "Epoch 11 Batch 7 Loss 1.0347\n",
      "Epoch 11 Batch 8 Loss 1.0371\n",
      "Epoch 11 Batch 9 Loss 1.0404\n",
      "Epoch 11 Batch 10 Loss 1.0410\n",
      "Epoch 11 Batch 11 Loss 1.0406\n",
      "Epoch 11 Loss 1.0406\n",
      "Time taken for 1 epoch: 271.5753502845764 secs\n",
      "\n",
      "Epoch 12 Batch 0 Loss 1.0661\n",
      "Epoch 12 Batch 1 Loss 1.0489\n",
      "Epoch 12 Batch 2 Loss 1.0342\n",
      "Epoch 12 Batch 3 Loss 1.0263\n",
      "Epoch 12 Batch 4 Loss 1.0255\n",
      "Epoch 12 Batch 5 Loss 1.0197\n",
      "Epoch 12 Batch 6 Loss 1.0249\n",
      "Epoch 12 Batch 7 Loss 1.0215\n",
      "Epoch 12 Batch 8 Loss 1.0258\n",
      "Epoch 12 Batch 9 Loss 1.0270\n",
      "Epoch 12 Batch 10 Loss 1.0323\n",
      "Epoch 12 Batch 11 Loss 1.0327\n",
      "Epoch 12 Loss 1.0327\n",
      "Time taken for 1 epoch: 270.9586977958679 secs\n",
      "\n",
      "Epoch 13 Batch 0 Loss 1.0235\n",
      "Epoch 13 Batch 1 Loss 1.0233\n",
      "Epoch 13 Batch 2 Loss 1.0143\n",
      "Epoch 13 Batch 3 Loss 1.0124\n",
      "Epoch 13 Batch 4 Loss 1.0198\n",
      "Epoch 13 Batch 5 Loss 1.0211\n",
      "Epoch 13 Batch 6 Loss 1.0179\n",
      "Epoch 13 Batch 7 Loss 1.0132\n",
      "Epoch 13 Batch 8 Loss 1.0137\n",
      "Epoch 13 Batch 9 Loss 1.0163\n",
      "Epoch 13 Batch 10 Loss 1.0199\n",
      "Epoch 13 Batch 11 Loss 1.0232\n",
      "Epoch 13 Loss 1.0232\n",
      "Time taken for 1 epoch: 270.878041267395 secs\n",
      "\n",
      "Epoch 14 Batch 0 Loss 0.9657\n",
      "Epoch 14 Batch 1 Loss 0.9816\n",
      "Epoch 14 Batch 2 Loss 0.9974\n",
      "Epoch 14 Batch 3 Loss 1.0008\n",
      "Epoch 14 Batch 4 Loss 1.0031\n",
      "Epoch 14 Batch 5 Loss 1.0078\n",
      "Epoch 14 Batch 6 Loss 1.0126\n",
      "Epoch 14 Batch 7 Loss 1.0132\n",
      "Epoch 14 Batch 8 Loss 1.0107\n",
      "Epoch 14 Batch 9 Loss 1.0136\n",
      "Epoch 14 Batch 10 Loss 1.0155\n",
      "Epoch 14 Batch 11 Loss 1.0137\n",
      "Epoch 14 Loss 1.0137\n",
      "Time taken for 1 epoch: 269.9937107563019 secs\n",
      "\n",
      "Epoch 15 Batch 0 Loss 0.9687\n",
      "Epoch 15 Batch 1 Loss 0.9951\n",
      "Epoch 15 Batch 2 Loss 0.9821\n",
      "Epoch 15 Batch 3 Loss 0.9871\n",
      "Epoch 15 Batch 4 Loss 0.9811\n",
      "Epoch 15 Batch 5 Loss 0.9828\n",
      "Epoch 15 Batch 6 Loss 0.9922\n",
      "Epoch 15 Batch 7 Loss 0.9936\n",
      "Epoch 15 Batch 8 Loss 0.9971\n",
      "Epoch 15 Batch 9 Loss 0.9997\n",
      "Epoch 15 Batch 10 Loss 1.0036\n",
      "Epoch 15 Batch 11 Loss 1.0029\n",
      "Saving checkpoint for epoch 15 at checkpoints\\ckpt-45\n",
      "Epoch 15 Loss 1.0029\n",
      "Time taken for 1 epoch: 272.0075578689575 secs\n",
      "\n",
      "Epoch 16 Batch 0 Loss 0.9699\n",
      "Epoch 16 Batch 1 Loss 0.9815\n",
      "Epoch 16 Batch 2 Loss 0.9832\n",
      "Epoch 16 Batch 3 Loss 0.9833\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    train_loss.reset_states()\n",
    "  \n",
    "    for (batch, (inp, tar)) in enumerate(dataset):\n",
    "        train_step(inp, tar)\n",
    "    \n",
    "        print ('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, train_loss.result()))\n",
    "      \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1, ckpt_save_path))\n",
    "    \n",
    "    print ('Epoch {} Loss {:.4f}'.format(epoch + 1, train_loss.result()))\n",
    "\n",
    "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PVbEUCZagJ0G"
   },
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YMbqGTixu1cl"
   },
   "source": [
    "#### Predicting one word at a time at the decoder and appending it to the output; then taking the complete sequence as an input to the decoder and repeating until maxlen or stop keyword appears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F5D5cv2Jd8-6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evaluate(input_document):\n",
    "    input_document = document_tokenizer.texts_to_sequences([input_document])\n",
    "    input_document = tf.keras.preprocessing.sequence.pad_sequences(input_document, maxlen=encoder_maxlen, padding='post', truncating='post')\n",
    "\n",
    "    encoder_input = tf.expand_dims(input_document[0], 0)\n",
    "\n",
    "    decoder_input = [summary_tokenizer.word_index[\"<go>\"]]\n",
    "    output = tf.expand_dims(decoder_input, 0)\n",
    "    \n",
    "    for i in range(decoder_maxlen):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)\n",
    "\n",
    "        predictions, attention_weights = transformer(\n",
    "            encoder_input, \n",
    "            output,\n",
    "            False,\n",
    "            enc_padding_mask,\n",
    "            combined_mask,\n",
    "            dec_padding_mask\n",
    "        )\n",
    "\n",
    "        predictions = predictions[: ,-1:, :]\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "        if predicted_id == summary_tokenizer.word_index[\"<stop>\"]:\n",
    "            return tf.squeeze(output, axis=0), attention_weights\n",
    "\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output, axis=0), attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UkpdiW6wnmiS"
   },
   "outputs": [],
   "source": [
    "def summarize(input_document):\n",
    "    # not considering attention weights for now, can be used to plot attention heatmaps in the future\n",
    "    summarized = evaluate(input_document=input_document)[0].numpy()\n",
    "    summarized = np.expand_dims(summarized[1:], 0)  # not printing <go> token\n",
    "    return summary_tokenizer.sequences_to_texts(summarized)[0]  # since there is just one translated document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[72]\n",
      "tf.Tensor([[72]], shape=(1, 1), dtype=int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'a asked there a lot of places to camping in dallas area we have one of b said not done some of camping in the best way to camping in the lawn and arkansas b said that b have done camping in the last september just had b enjoy camping when b room and garden and weeds b lived in the lake b did not had had b have done camping b said it is really beautiful beautiful b said it is really had had had had to grow b said that when b have done a said that sounds fun b have done a lot of places b said we have done ours a have done camping twice b said wow a really kind of camping grounds where you have done this country it was there and stuff a have never had a lot more fun to go out a lot of doing camping grounds where you got r v beside you and then another people in tents beside you a just do not care of places b said no a have done camping b said no a said no a said no a said that nature a said no a said that people who did not care of a said no always kills us see that a grill seen this old refrigerator that somebody started the last time the a was at a lot of hiking which held our pan and put it over our hole b said what a clever idea b would not have had a said that a said it held our pan and the potatoes and stuff we had a good idea a said that is a good idea a said we are still waiting to go a very nice to go out doing anything a lot of the wichitas which is a said that would love to b would just started in the drive up in oklahoma city b have some kind of stuff we did b take and arkansas has b take share b camp a place as a drive up in the drive through about b know b have some of the bathroom and some kind of lakes were b camp the edge b said that b know that b said that b would b have a lot of dallas b would not have people were b have had to drive to drive up here b cannot share b camp the edge b would little bit b would not had to the edge b would had b camp a mcdonalds b cannot even'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f=open(\"C:/Users/91908/Desktop/Mini Project/Test Data/text/test1.txt\",\"r\")\n",
    "i=f.read()\n",
    "fs=summarize(i)\n",
    "fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "data_plot = pd.DataFrame({\"Epoch\":epoch, \"Loss\":loss})\n",
    " \n",
    "sns.lineplot(x = \"Epoch\", y = \"Loss\", data=data_plot)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyN7AJp0I1extht6swPTmZxm",
   "collapsed_sections": [],
   "mount_file_id": "1P1vi4p4uje5OlCp9bdNqlbdxqUCX-qPJ",
   "name": "summarizer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
